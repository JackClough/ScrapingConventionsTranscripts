{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping US National Convention Transcripts\n",
    "\n",
    "This notebook demonstrates how to scrape and process transcripts from the Democratic and Republican national conventions, using transcript pages provided by Rev.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from random import random\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "We define helpers to extract visible text from HTML and to generate filenames from URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Comment\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def generate_filename_from_url(url):\n",
    "    if not url:\n",
    "        return None\n",
    "    name = url.replace(\"https\", \"\").replace(\"http\", \"\")\n",
    "    name = name.replace(\"://\", \"\").replace(\".\", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "    last_underscore_spot = name.rfind(\"_\")\n",
    "    name = name[:last_underscore_spot] + name[(last_underscore_spot+1):]\n",
    "    name = name + \".txt\"\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define convention transcript URLs for both parties\n",
    "\n",
    "convention_pages = {\n",
    "    \"democrats\": [\n",
    "        \"https://www.rev.com/blog/transcripts/democratic-national-convention-dnc-night-1-transcript\",\n",
    "        \"https://www.rev.com/blog/transcripts/democratic-national-convention-dnc-2020-night-2-transcript\",\n",
    "        \"https://www.rev.com/blog/transcripts/democratic-national-convention-dnc-night-3-transcript\",\n",
    "        \"https://www.rev.com/blog/transcripts/2020-democratic-national-convention-dnc-night-4-transcript\"\n",
    "    ],\n",
    "    \"republicans\": [\n",
    "        \"https://www.rev.com/blog/transcripts/2020-republican-national-convention-rnc-night-1-transcript\",\n",
    "        \"https://www.rev.com/blog/transcripts/2020-republican-national-convention-rnc-night-2-transcript\",\n",
    "        \"https://www.rev.com/blog/transcripts/2020-republican-national-convention-rnc-night-3-transcript\",\n",
    "        \"https://www.rev.com/blog/transcripts/2020-republican-national-convention-rnc-night-4-transcript\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "\n",
    "- `convention_pages` is a dictionary mapping party names to lists of transcript URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping and Saving Transcripts\n",
    "\n",
    "For each transcript URL, we fetch the page, extract visible text, and save the result to a text file named after the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 10.70 seconds.\n",
      "Waiting for 7.50 seconds.\n",
      "Waiting for 14.00 seconds.\n",
      "Waiting for 5.82 seconds.\n",
      "Waiting for 9.49 seconds.\n",
      "Waiting for 13.10 seconds.\n",
      "Waiting for 5.84 seconds.\n",
      "Waiting for 12.70 seconds.\n"
     ]
    }
   ],
   "source": [
    "def scrape_and_save_transcripts(convention_pages):\n",
    "    for party, links in convention_pages.items():\n",
    "        for url in links:\n",
    "            try:\n",
    "                r = requests.get(url)\n",
    "                if r.status_code == 200:\n",
    "                    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "                    texts = soup.findAll(text=True)\n",
    "                    visible_texts = filter(tag_visible, texts)\n",
    "                    page_text = \" \".join(t.strip() for t in visible_texts)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {url} (status code {r.status_code})\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            output_file_name = generate_filename_from_url(url)\n",
    "            # Clean up text for output\n",
    "            page_text = page_text.replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "            with open(output_file_name, 'w', encoding=\"UTF-8\") as outfile:\n",
    "                outfile.write(\"link\\ttext\\n\")\n",
    "                outfile.write(f\"{url}\\t{page_text}\\n\")\n",
    "            wait_time = 5 + random() * 10\n",
    "            print(f\"Saved {output_file_name}. Waiting for {wait_time:.02f} seconds.\")\n",
    "            sleep(wait_time)\n",
    "\n",
    "scrape_and_save_transcripts(convention_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example: Wrapping Long Text Output\n",
    "\n",
    "When writing out long strings, it's helpful to wrap the text for readability. The `textwrap` library makes this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices, seed\n",
    "from string import ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a long string with spaces for demonstration\n",
    "\n",
    "string_length = 50000\n",
    "chars_to_sample = ascii_letters + \" \" * 8\n",
    "\n",
    "seed(20200916)\n",
    "text = \"\".join(choices(chars_to_sample, k=string_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the text as a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text.txt\", 'w') as outfile:\n",
    "    outfile.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, wrap the text for easier reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_text = textwrap.wrap(text)\n",
    "\n",
    "with open(\"text_wrapped.txt\", 'w') as outfile:\n",
    "    for piece in wrapped_text:\n",
    "        outfile.write(piece + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
